global:
    dbCredentials:
        pool_password: changeme
        pool_user: pgpool
        postgres_db: default
        postgres_password: changeme
        postgres_user: postgres
        replication_user: replication
    env: TESTING
    existingClaim: {}
    ingress:
        class: nginx
        enabled: true
    masterReplicas: 1
    namespace: vulnerability-assessment-tool-core
    podPriorityClass:
        enabled: true
    projectName: vulnerability-assessment-tool
    replicationFactor: 2
    slaveReplicas: 2

postgres:
  metrics:
    enabled: true

  masterAntiAffinity:
    soft: true
    mastermasterAntiaffinity:  100
    masterslaveAntiaffinity: 50

  slaveAntiAffinity:
    soft: true
    weight: 50

  # Values in {
  # - DEBUG1..DEBUG5
  # - INFO
  # - NOTICE
  # - WARNING
  # - ERROR
  # - LOG
  # - FATAL
  # - PANIC
  # }
  debug: WARNING

  image:
    initContainer:
      pullPolicy: "IfNotPresent"
      registry: {}
      registryPort: {}
      name: "postgres"
      # Alpine images for init container to reduce
      # overal resource strain with Image size: 28MBs
      tag: "11.5-alpine"

      securityContext:
        runAsUser: 0
        privileged: true

      resources:
        limits:
          memory: "35Mi"
          cpu: "150m"
        requests:
          memory: "25Mi"
          cpu: "100m"

    mainContainer:
      pullPolicy: "IfNotPresent"
      registry: {}
      registryPort: {}
      name: "postgres"
      # Debian based image
      # Image size : 117MB
      tag: "11.5"

      securityContext:
        runAsUser: 999
        runAsGroup: 999
        privileged: false
        readOnlyRootFilesystem: false

    exporterContainer:
      pullPolicy: "IfNotPresent"
      registry: {}
      registryPort: {}
      name: "wrouesnel/postgres_exporter"
      # Image size : 5.65MB
      tag: "v0.5.1"

  master:
    extraConfigs: {}
    updateStrategy: "RollingUpdate"
    # Suggestion : ~= 1
    #              or multiple but requires strict separation of R/W stream
    #                 due to lack of consistency between masters
    replicas: {}
    resources:
      requests:
        memory: "15Gi"
        cpu: "15"
      limits:
        memory: "22Gi"
        cpu: "22"

    podDisruptionBudget:
      #    Warning :  this won't be applied unless the replicas
      #               values are >= 2
      minAvailable: 1

    podPriorityClass:
      # If .Values.global.podPriorityClass.enabled and spec is {}
      # defaults to
      #   globalDefault: false
      #   value: 100000
      #   preemptionPolicy: PreemptLowerPriority
      spec: {}

    # Allows pods to live a certain period after termination
    terminationGracePeriodSeconds: 10

    readinessProbe:
      enabled: true
      # Worst case scenario time of failed response before container is considered ready by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 29s ~= .5min
      initialDelaySeconds: 5
      periodSeconds: 20
      timeoutSeconds: 3
      failureThreshold: 3

    livenessProbe:
      enabled: true
      # Worst case scenario time of failed response before container is killed by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 220s ~= 4min
      initialDelaySeconds: 60
      periodSeconds: 20
      timeoutSeconds: 3
      failureThreshold: 20

    # This value is injected to postgres using the env variable $PG_DATA
    # and the default value is the one recommended for postgresql imaes
    pvcMountPath: "/var/lib/postgresql/data"
    accessModes: "ReadWriteOnce"
    requests:
      # This value is set for production conditions and is therefore quite high
      storage: "10Gi"

  slave:
    backoffDuration: 10
    resources:
      requests:
        memory: "8Gi"
        cpu: "8"
      limits:
        memory: "15Gi"
        cpu: "15"

    extraConfigs: {}
    # Warning: this won't be applied unless the replicas
    #          values are >= 2
    podDisruptionBudget: {}
    podPriorityClass:
      # If .Values.global.podPriorityClass.enabled and spec is {}
      # defaults to
      #   globalDefault: false
      #   value: 40000
      #   preemptionPolicy: PreemptLowerPriority
      spec: {}

    # Replicas will be dynamically calculated if set to {}
    # # Desired slave statefulset replicas
    # # Suggestion : ~= R * master stateful set replicas
    # #                 with R the desired replication factor
    # # Replicas will be dynamically calculated if set to {}
    # replicas: 3

    # Allows for automated rolling updates
    updateStrategy: "RollingUpdate"

    # Allows pods to live a certain period after termination
    terminationGracePeriodSeconds: 10
    readinessProbe:
      enabled: true
      # Worst case scenario time of failed response before container is considered ready by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 94s ~= 1.5min
      initialDelaySeconds: 40
      periodSeconds: 15
      timeoutSeconds: 3
      failureThreshold: 3

    livenessProbe:
      # Worst case scenario time of failed response before container is killed by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 220s ~= 4min
      enabled: false
      initialDelaySeconds: 40
      periodSeconds: 15
      timeoutSeconds: 3
      failureThreshold: 10

    # This value is injected to postgres using the env variable $PG_DATA
    # and the default value is the one recommended for postgresql imaes
    pvcMountPath: "/var/lib/postgresql/data"
    accessModes: "ReadWriteOnce"
    requests:
      # This value is set for production conditions and is therefore quite high
      storage: "10Gi"

pgpool:
  name: pgpool
  # If set to false this sets CRUNCHY_DEBUG="False" as an env variable
  # else CRUNCHY_DEBUG="true" because this is a crunchy pgpool image
  debug: false
  # Suggestion : ~= master statefulset replicas + slave statefulset replicas
  # dynamically computed if set to {}
  replicas: 3

  podPriorityClass:
    # If .Values.global.podPriorityClass.enabled and spec is {}
    # defaults to
    #   globalDefault: false
    #   value: 30000
    #   preemptionPolicy: PreemptLowerPriority
    spec: {}

  # Allows for automated rolling updates
  updateStrategy: "RollingUpdate"
  podDisruptionBudget:
    #    Warning :  this won't be applied unless the replicas
    #               values are >= 2
    # Pod disruption budget can be set dynamically or statically
    # dynamic: sum ( masters + replication factor * master )
    dynamic: true
    # minAvailable: 1

  antiAffinity:
    soft: true
    selfAntiAffinity: 50

  affinity:
    soft: true
    # affinity towards being close to postgres replica nodes
    poolreplicaAffinity: 50
    # affinity towards being close to postgres master nodes
    # preference higher than replica nodes because write functions
    # are only available on master nodes
    poolmasterAffinity: 70

  loadBalanceMode: on
  failOverOnBackendError: off
  numInitChildren: 100

  maxPool: 10
  clientIdleLimit: 920
  connectionLifeTime: 500

  # Allows pods to live a certain period after termination
  terminationGracePeriodSeconds: 15
  useWatchDog: on
  watchDog:
    interval: 15

  healthCheck:
    timeout: 0
    period: 30
    maxRetries: 5
    retryDelay: 5

  image:
    pullPolicy: "IfNotPresent"
    registry: {}
    registryPort: {}
    name: "crunchydata/crunchy-pgpool"
    tag: "centos7-11.4-2.4.1"

    securityContext:
      runAsUser: 999
      runAsGroup: 999
      privileged: false
      readOnlyRootFilesystem: false

    resources:
      limits:
        memory: "2Gi"
        cpu: "2000m"
      requests:
        memory: "1Gi"
        cpu: "1000m"

    readinessProbe:
      # Worst case scenario time of failed response before container is considered ready by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 95s ~= 1.5min
      enabled: true
      initialDelaySeconds: 35
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 3

    livenessProbe:
      # Worst case scenario time before container is killed by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 155s ~= 2.5min
      enabled: true
      initialDelaySeconds: 35
      periodSeconds: 30
      timeoutSeconds: 10
      failureThreshold: 3
